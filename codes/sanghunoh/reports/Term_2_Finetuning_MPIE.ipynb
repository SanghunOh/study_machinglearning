{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Term_2_Finetuning_MPIE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/SanghunOh/study_machinglearning/blob/main/codes/sanghunoh/reports/Term_2_CNN_MPIE.ipynb",
      "authorship_tag": "ABX9TyOVc4Cf29WrFr+H3225eIFn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanghunOh/study_machinglearning/blob/main/codes/sanghunoh/reports/Term_2_Finetuning_MPIE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb?force_kitty_mode=1&force_corgi_mode=1#scrollTo=Wv4afXKj6cVa"
      ],
      "metadata": {
        "id": "ELXUwKUWSuwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Connect Drive"
      ],
      "metadata": {
        "id": "brb_gNSQ-Tyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "foY3SJ1joaVf",
        "outputId": "6425fde8-bb9f-43ca-9fcf-16d40081adb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zl8YlT9398C"
      },
      "outputs": [],
      "source": [
        "path_root = '/content/drive/MyDrive/datas/'\n",
        "\n",
        "# 작업 경로 설정\n",
        "import os\n",
        "os.chdir(path_root)\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -o ./mpie_30_shuffle.zip -d ./mpie_30_shuffle"
      ],
      "metadata": {
        "id": "q_zK6dD69lx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "mXG7KRQz-Nax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Dataset"
      ],
      "metadata": {
        "id": "qLUE0VPqLewv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import reshape\n",
        "path = path_root + 'mpie_30_shuffle/'\n",
        "\n",
        "def loadDatasetFromCSV(_datafilename, _labelfilename, label_column, rgb_expand=False):\n",
        "  _feature_csv = pd.read_csv(path + _datafilename, dtype=np.float32, header=None) # image features of train data\n",
        "  _feature_flatten = _feature_csv.values.flatten()\n",
        "\n",
        "  _feature_reshape = np.reshape(_feature_flatten, (_feature_csv.shape[0], 32, 32))\n",
        "  if rgb_expand:\n",
        "    _feature_reshape = np.repeat(_feature_reshape[..., np.newaxis], 3, -1)    \n",
        "  print(_feature_reshape.shape)\n",
        "  _label_csv = pd.read_csv(path + _labelfilename, dtype=np.float32, header=None) # labels of train data\n",
        "  # print(f'_label_csv : {_label_csv.shape}')\n",
        "  _label = _label_csv[label_column] # get label you want\n",
        "\n",
        "  return _feature_reshape, _label"
      ],
      "metadata": {
        "id": "wzRrppGEyLWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# train\n",
        "train_feature_rgb, train_label = loadDatasetFromCSV('Traindata.csv', 'Trainlabel.csv', 0, rgb_expand=True)\n",
        "# test\n",
        "validation_feature_rgb, validation_label = loadDatasetFromCSV('Testdata.csv', 'Testlabel.csv', 0, rgb_expand=True)\n",
        "\n",
        "train_feature_rgb.shape, train_label.shape, validation_feature_rgb.shape, validation_label.shape"
      ],
      "metadata": {
        "id": "bLAz9qj1LwGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.unique(train_label)"
      ],
      "metadata": {
        "id": "m5ckZH8cYAcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjUDklviz51u"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def preprocessDataset(features_reshape, labels, normalization=False, batch_size=64, shuffle_buffer_size=100):\n",
        "  _batch_size = batch_size\n",
        "  _shuffle_buffer_size = shuffle_buffer_size\n",
        "\n",
        "  normal_value = 1\n",
        "  if normalization:\n",
        "    normal_value = 255\n",
        "  _features = features_reshape / normal_value\n",
        "  _labels = labels\n",
        "  _dataset_tensors = tf.data.Dataset.from_tensor_slices((_features, _labels))\n",
        "\n",
        "  if _shuffle_buffer_size == None:\n",
        "    _dataset_tensors = _dataset_tensors.batch(_batch_size)\n",
        "  else :\n",
        "    _dataset_tensors = _dataset_tensors.shuffle(_shuffle_buffer_size).batch(_batch_size)\n",
        "\n",
        "  return _dataset_tensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size\n",
        "train_label_length = len(train_label)\n",
        "print(train_label_length)\n",
        "batch_size = int(train_label_length / (train_label_length/9))\n",
        "\n",
        "train_dataset_rgb = preprocessDataset(train_feature_rgb, train_label, batch_size=batch_size)\n",
        "train_dataset_rgb"
      ],
      "metadata": {
        "id": "b0iSCMgGOjCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset_rgb?\n",
        "# train_dataset_rgb.element_spec\n",
        "train_dataset_rgb.element_spec"
      ],
      "metadata": {
        "id": "7CJbD5hHSYCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset_rgb.take(1):\n",
        "  batch_count = labels.shape[0]\n",
        "  rows_cols = int(np.sqrt(batch_count))\n",
        "  print(batch_count, rows_cols)\n",
        "  for i in range(batch_count):\n",
        "    ax = plt.subplot(rows_cols, rows_cols, i + 1)\n",
        "    plt.imshow(images[i].numpy(), cmap='gray')\n",
        "    plt.title(int(labels[i].numpy()))\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "WjIC9BwIpJyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use data augmentation"
      ],
      "metadata": {
        "id": "EEqyi-CZn1jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "  tf.keras.layers.Resizing(height=96,width=96)\n",
        "])"
      ],
      "metadata": {
        "id": "Sse8GvsH0Rlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for image, _ in train_dataset_rgb.take(1):\n",
        "  first_image = image[0]\n",
        "  batch_count = 16\n",
        "  rows_cols = int(np.sqrt(batch_count))\n",
        "  for i in range(batch_count):\n",
        "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "    print(f'{type(augmented_image[0].numpy().shape)}, {augmented_image[0].numpy().shape}')\n",
        "    ax = plt.subplot(rows_cols, rows_cols, i + 1)\n",
        "    plt.imshow(augmented_image[0], cmap='gray')\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "84zvlwGy0USD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##check pre-trained model"
      ],
      "metadata": {
        "id": "-feC6g_D2FpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset_rgb.shape[1:]"
      ],
      "metadata": {
        "id": "Z-_ZAI0Z2z0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_count = len(train_label.unique())\n",
        "class_count"
      ],
      "metadata": {
        "id": "9g3ot7K9Rd06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  tensorflow.keras import layers\n",
        "\n",
        "trained_model = tf.keras.applications.MobileNetV2(input_shape=(96,96,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "# trained_model.summary()"
      ],
      "metadata": {
        "id": "Uzjr5yEFWJzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.trainable = True\n",
        "fine_tune_at = int((1 - 0.1) * len(trained_model.layers))\n",
        "for layer in trained_model.layers[:fine_tune_at]:\n",
        "  layer.trainable = False\n",
        "  # print(layer.name)\n",
        "len(trained_model.trainable_variables)"
      ],
      "metadata": {
        "id": "vGo3nRk3qISO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int((1 - 0.1) * len(trained_model.layers))"
      ],
      "metadata": {
        "id": "pIkczPfcvMAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.output    # without classfication layers"
      ],
      "metadata": {
        "id": "xJWagignS7xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.trainable = False\n",
        "# trained_model.summary()"
      ],
      "metadata": {
        "id": "UuxVfjyQRyXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##make model from pre-trained model"
      ],
      "metadata": {
        "id": "EdYoDXzxOfX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = train_feature_rgb.shape[1:]\n",
        "print(image_shape)"
      ],
      "metadata": {
        "id": "sKceRufmlfoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create the base model from the pre-trained models\n",
        "def make_model(_class_cnt, _learning_rate=0.0001,_logits_flag=True, fine_tune_at=0.0\n",
        "               , trained_model_name ='MobileNetV2', epochs=2, label_class=0):\n",
        "\n",
        "  if trained_model_name == 'MobileNetV2':\n",
        "    _trained_model = tf.keras.applications.MobileNetV2(input_shape=(96,96,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "  inputs = layers.Input(shape=(32,32,3), name='custom_input')\n",
        "  augment_input = data_augmentation(inputs)\n",
        "  add_layer = _trained_model(augment_input, training=False)\n",
        "  add_layer = tf.keras.layers.GlobalAveragePooling2D()(add_layer)\n",
        "  add_layer = tf.keras.layers.Dropout(0.2)(add_layer)\n",
        "  outputs = tf.keras.layers.Dense(_class_cnt)(add_layer)\n",
        "  _model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  _model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=_learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=_logits_flag),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  _trained_model.trainable = True\n",
        "  fine_tune_at = int((1 - 0.1) * len(_trained_model.layers))\n",
        "  for layer in _trained_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "    # print(layer.name)\n",
        "  # len(trained_model.trainable_variables)\n",
        "  return _model\n"
      ],
      "metadata": {
        "id": "SJYcVF9w2FVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_model(2,fine_tune_at=0.1)\n",
        "# model.summary()\n",
        "len(model.trainable_variables)"
      ],
      "metadata": {
        "id": "CcTewQRyal2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = preprocessDataset(validation_feature_rgb, validation_label, batch_size=batch_size, shuffle_buffer_size=None)\n",
        "validation_dataset"
      ],
      "metadata": {
        "id": "zrUhEcp4PEk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##callback function for fit time"
      ],
      "metadata": {
        "id": "69PhSLjqmI8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "TimeHistory()"
      ],
      "metadata": {
        "id": "-r_M426PmP5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fit model"
      ],
      "metadata": {
        "id": "1E1Fdq6pnRAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows_length = train_feature_rgb.shape[0]\n",
        "# steps_per_epoch = int(rows_length / (rows_length))\n",
        "steps_per_epoch = int(rows_length / (rows_length/5))\n",
        "\n",
        "def model_fit(_param, _train_dataset, _validation_dataset):\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  _model = make_model(**_param)\n",
        "  time_callback = TimeHistory()\n",
        "  _history = _model.fit(_train_dataset, epochs=_param['epochs'], validation_data=_validation_dataset\n",
        "                      , callbacks=[time_callback], steps_per_epoch=steps_per_epoch)\n",
        "  _execution_time = sum(time_callback.times)\n",
        "  return _model, _history, _execution_time, _param"
      ],
      "metadata": {
        "id": "bNVwwpiExuY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_class = 0\n",
        "# train\n",
        "train_feature_rgb, train_label = loadDatasetFromCSV('Traindata.csv', 'Trainlabel.csv', label_class, rgb_expand=True)\n",
        "class_cnt = len(np.unique(train_label))\n",
        "# test\n",
        "validation_feature_rgb, validation_label = loadDatasetFromCSV('Testdata.csv', 'Testlabel.csv', label_class, rgb_expand=True)\n",
        "\n",
        "train_dataset_rgb = preprocessDataset(train_feature_rgb, train_label)\n",
        "validation_dataset = preprocessDataset(validation_feature_rgb, validation_label, shuffle_buffer_size=None)\n",
        "\n",
        "param = {'_class_cnt':class_cnt, '_learning_rate':0.0001,'_logits_flag':True, 'fine_tune_at':0.0\n",
        "         , 'trained_model_name':'MobileNetV2', 'epochs':2, 'label_class':0}\n",
        "\n",
        "model_fit(param, train_dataset_rgb, validation_dataset)"
      ],
      "metadata": {
        "id": "kU1uwqf8NyEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fit with multi params"
      ],
      "metadata": {
        "id": "6NzJoF7tx9Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param = {'_class_cnt':class_cnt, '_learning_rate':0.0001,'_logits_flag':True, 'fine_tune_at':0.0\n",
        "         , 'trained_model_name':'MobileNetV2', 'epochs':2, 'label_class':0}"
      ],
      "metadata": {
        "id": "2rWo9r9qQhWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# param 6\n",
        "params = list()\n",
        "param_key_list = list(param.keys())\n",
        "epochs_list = [10]\n",
        "_learning_rate_list = [0.0001, 0.01]\n",
        "_logits_flag_list = [True, False]\n",
        "fine_tune_at_list = [0.0, 0.1]\n",
        "\n",
        "for _learning_rate in _learning_rate_list:\n",
        "  for _logits_flag in _logits_flag_list:\n",
        "    for fine_tune_at in fine_tune_at_list:\n",
        "      for epochs in epochs_list:\n",
        "        param_dict = dict()\n",
        "        param_dict['_learning_rate'] = _learning_rate\n",
        "        param_dict['_logits_flag'] = _logits_flag\n",
        "        param_dict['fine_tune_at'] = fine_tune_at\n",
        "        param_dict['epochs'] = epochs\n",
        "        # print(param_dict)\n",
        "        params.append(param_dict)\n",
        "len(params)"
      ],
      "metadata": {
        "id": "Qv6FmRH5RJFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# datetime object containing current date and time\n",
        "now = datetime.now()\n",
        " \n",
        "print(\"now =\", now)\n",
        "\n",
        "# dd/mm/YY H:M:S\n",
        "dt_string = now.strftime(\"%Y%m%d%H%M\")\n",
        "print(\"date and time =\", dt_string)\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "9TTq4ChAI015"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# params = params[5:7]\n",
        "label_classes = [1, 2, 4]\n",
        "results_list = list()\n",
        "for label_class in label_classes:\n",
        "  train_feature_rgb, train_label = loadDatasetFromCSV('Traindata.csv', 'Trainlabel.csv', label_class, rgb_expand=True)\n",
        "  class_cnt = len(np.unique(train_label))\n",
        "  # test\n",
        "  validation_feature_rgb, validation_label = loadDatasetFromCSV('Testdata.csv', 'Testlabel.csv', label_class, rgb_expand=True)\n",
        "\n",
        "  train_dataset_rgb = preprocessDataset(train_feature_rgb, train_label)\n",
        "  validation_dataset = preprocessDataset(validation_feature_rgb, validation_label, shuffle_buffer_size=None)\n",
        "\n",
        "  results = list()          \n",
        "  for idx, param in enumerate(params):\n",
        "    param['_class_cnt'] = class_cnt\n",
        "    param['label_class'] = label_class\n",
        "    print('-'*5 +'[ '+ str(idx) + ' ]'+ '-'*5 + str(param))\n",
        "    results.append(model_fit(param, train_dataset_rgb, validation_dataset))\n",
        "  # save\n",
        "  with open(f'results_Finetuning_label_{label_class}_{dt_string}.pickle', 'wb') as f:\n",
        "    pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)\n",
        "  results_list = results_list + results\n"
      ],
      "metadata": {
        "id": "i1Tnuhc2x5eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save\n",
        "with open(f'results_Finetuning_{dt_string}.pickle', 'wb') as f:\n",
        "    pickle.dump(results_list, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "nAdPQV3EeZhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}