{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Term_2_Finetuning_MPIE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/SanghunOh/study_machinglearning/blob/main/codes/sanghunoh/reports/Term_2_CNN_MPIE.ipynb",
      "authorship_tag": "ABX9TyOW50DGTwfLNRn28D3aBcZr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanghunOh/study_machinglearning/blob/main/codes/sanghunoh/reports/Term_2_Finetuning_MPIE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Connect Drive"
      ],
      "metadata": {
        "id": "brb_gNSQ-Tyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zl8YlT9398C",
        "outputId": "30311fdd-9b24-4c3d-8387-16c8e9d36454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1tHYgEz3lQF-LmtFPXRgMBDh4UN54qeAs/datas\n"
          ]
        }
      ],
      "source": [
        "path_root = '/content/drive/MyDrive/datas/'\n",
        "\n",
        "# 작업 경로 설정\n",
        "import os\n",
        "os.chdir(path_root)\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -o ./mpie_30_shuffle.zip -d ./mpie_30_shuffle"
      ],
      "metadata": {
        "id": "q_zK6dD69lx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXG7KRQz-Nax",
        "outputId": "4f50544a-7a52-4c83-87fa-9e7be4efb926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataCh4_7.mat\t\tiris_shuffle.mat\t mpie_30_shuffle\n",
            "dataCh4_7.zip\t\tiris.zip\t\t mpie_30_shuffle.zip\n",
            "digitimages_binary.zip\tmatlab_iris_shuffle.mat  number_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Dataset"
      ],
      "metadata": {
        "id": "qLUE0VPqLewv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = path_root + 'mpie_30_shuffle/'\n",
        "\n",
        "def loadDatasetFromCSV(_datafilename, _labelfilename, label_column):\n",
        "  _feature_csv = pd.read_csv(path + _datafilename, dtype=np.float32, header=None) # image features of train data\n",
        "  _feature_flatten = _feature_csv.values.flatten()\n",
        "  _feature_reshape = np.reshape(_feature_flatten, (_feature_csv.shape[0], 32,-1))\n",
        "\n",
        "  _label_csv = pd.read_csv(path + _labelfilename, dtype=np.float32, header=None) # labels of train data\n",
        "  # print(f'_label_csv : {_label_csv.shape}')\n",
        "  _label = _label_csv[label_column] # get label you want\n",
        "\n",
        "  return _feature_reshape, _label"
      ],
      "metadata": {
        "id": "wzRrppGEyLWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# train\n",
        "train_feature_reshape, train_label = loadDatasetFromCSV('Traindata.csv', 'Trainlabel.csv', 0)\n",
        "# test\n",
        "validation_feature_reshape, validation_label = loadDatasetFromCSV('Testdata.csv', 'Testlabel.csv', 0)\n",
        "\n",
        "train_feature_reshape.shape, train_label.shape, validation_feature_reshape.shape, validation_label.shape"
      ],
      "metadata": {
        "id": "bLAz9qj1LwGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995dd2d6-80a5-406f-8b68-39d0f265b5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((18777, 32, 32), (18777,), (5086, 32, 32), (5086,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(train_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5ckZH8cYAcL",
        "outputId": "038c224f-6314-4f7f-cfc9-781f77841b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
              "       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
              "       26., 27., 28., 29.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjUDklviz51u"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def preprocessDataset(features_reshape, labels, batch_size=64, shuffle_buffer_size=100):\n",
        "  _batch_size = batch_size\n",
        "  _shuffle_buffer_size = shuffle_buffer_size\n",
        "\n",
        "  _features = features_reshape\n",
        "  _labels = labels\n",
        "  _dataset_tensors = tf.data.Dataset.from_tensor_slices((_features, _labels))\n",
        "\n",
        "  if _shuffle_buffer_size == None:\n",
        "    _dataset_tensors = _dataset_tensors.batch(_batch_size)\n",
        "  else :\n",
        "    _dataset_tensors = _dataset_tensors.shuffle(_shuffle_buffer_size).batch(_batch_size)\n",
        "\n",
        "  return _dataset_tensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = preprocessDataset(train_feature_reshape, train_label)\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "b0iSCMgGOjCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c427fcd5-2200-40d3-b72c-cc99a6b02330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 32, 32), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset?"
      ],
      "metadata": {
        "id": "7CJbD5hHSYCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = preprocessDataset(validation_feature_reshape, validation_label, shuffle_buffer_size=None)\n",
        "validation_dataset"
      ],
      "metadata": {
        "id": "zrUhEcp4PEk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0023de32-8568-4f29-9b5d-441f301b58f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 32, 32), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EEqyi-CZn1jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###callback function for fit time"
      ],
      "metadata": {
        "id": "69PhSLjqmI8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "TimeHistory()"
      ],
      "metadata": {
        "id": "-r_M426PmP5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9739a235-7f1f-480e-99d9-509e8cb26777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.TimeHistory at 0x7f8bec93d490>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fit model"
      ],
      "metadata": {
        "id": "1E1Fdq6pnRAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_epochs = 10\n",
        "_batch_size = int(len(list(train_dataset)) / (len(list(train_dataset))/2))   # Just Check Model params quickly\n",
        "# _batch_size = len(list(train_dataset))\n",
        "\n",
        "def model_fit(_param, _train_dataset, _validation_dataset):\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  model = make_model(**_param)\n",
        "  time_callback = TimeHistory()\n",
        "  history = model.fit(_train_dataset, epochs=_param['epochs'], validation_data=_validation_dataset, callbacks=[time_callback], steps_per_epoch=_batch_size)\n",
        "  execution_time = sum(time_callback.times)\n",
        "  return model, history, execution_time, _param"
      ],
      "metadata": {
        "id": "QYn7Q8f311Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_class = 0\n",
        "# train\n",
        "train_feature_reshape, train_label = loadDatasetFromCSV('Traindata.csv', 'Trainlabel.csv', label_class)\n",
        "class_cnt = len(np.unique(train_label))\n",
        "# test\n",
        "validation_feature_reshape, validation_label = loadDatasetFromCSV('Testdata.csv', 'Testlabel.csv', label_class)\n",
        "\n",
        "train_dataset = preprocessDataset(train_feature_reshape, train_label)\n",
        "validation_dataset = preprocessDataset(validation_feature_reshape, validation_label, shuffle_buffer_size=None)\n",
        "\n",
        "param = {'_class_cnt':class_cnt, 'epochs':2, 'label_class':label_class, '_kernel_initializer':None\n",
        "         , '_activation':'sigmoid', 'batch_normal':False, '_optimizer':'RMSprop', 'filters':16 , '_padding':'valid'\n",
        "         , 'pool_type':'max', '_kernel_size': (3,3), '_strides': (1,1), '_pool_size': (2,2)}\n",
        "\n",
        "model_fit(param, train_dataset, validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU1uwqf8NyEt",
        "outputId": "ce65685a-f236-45fe-ba8a-acd6952fc861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2/2 [==============================] - 4s 2s/step - loss: 3.9965 - accuracy: 0.0234 - val_loss: 3.6237 - val_accuracy: 0.0203\n",
            "Epoch 2/2\n",
            "2/2 [==============================] - 2s 2s/step - loss: 3.8177 - accuracy: 0.0312 - val_loss: 3.5895 - val_accuracy: 0.0203\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<keras.engine.sequential.Sequential at 0x7f8bec93db50>,\n",
              " <keras.callbacks.History at 0x7f8bec9b6510>,\n",
              " 6.299786567687988,\n",
              " {'_activation': 'sigmoid',\n",
              "  '_class_cnt': 30,\n",
              "  '_kernel_initializer': None,\n",
              "  '_kernel_size': (3, 3),\n",
              "  '_optimizer': 'RMSprop',\n",
              "  '_padding': 'valid',\n",
              "  '_pool_size': (2, 2),\n",
              "  '_strides': (1, 1),\n",
              "  'batch_normal': False,\n",
              "  'epochs': 2,\n",
              "  'filters': 16,\n",
              "  'label_class': 0,\n",
              "  'pool_type': 'max'})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fit with multi params\n",
        "filters : larger than 16"
      ],
      "metadata": {
        "id": "6NzJoF7tx9Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param = {'_class_cnt':class_cnt, 'epochs':2, 'label_class':label_class, '_kernel_initializer':None\n",
        "         , '_activation':'sigmoid', 'batch_normal':False, '_optimizer':'Nadam', 'filters':16 , '_padding':'same'\n",
        "         , 'pool_type':'average', '_kernel_size': (3,3), '_strides': (1,1), '_pool_size': (2,2)}\n"
      ],
      "metadata": {
        "id": "2rWo9r9qQhWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# param 6\n",
        "params = list()\n",
        "param_key_list = list(param.keys())\n",
        "epochs_list = [10, 50, 100]\n",
        "_kernel_initializer_list = [None, 'glorot_uniform', 'he_normal']\n",
        "_activation_list = ['sigmoid', 'relu']\n",
        "_optimizer_list= ['Adam', 'Nadam']\n",
        "batch_normal_list = [False, True]\n",
        "filters_list = [16, 32]\n",
        "_padding_list = ['same', 'valid']\n",
        "_kernel_size_list = [(3,3), (5,5)]\n",
        "_strides_list = [(1,1), (3,3)]\n",
        "pool_type_list = ['average', 'max']\n",
        "\n",
        "for filters in filters_list:\n",
        "  for _padding in _padding_list:\n",
        "    for _kernel_initializer in _kernel_initializer_list:\n",
        "      for _activation in _activation_list:\n",
        "        for _optimizer in _optimizer_list:\n",
        "          for batch_normal in batch_normal_list:\n",
        "            for epochs in epochs_list:\n",
        "              for _kernel_size in _kernel_size_list:\n",
        "                for _strides in _strides_list:\n",
        "                  for pool_type in pool_type_list:\n",
        "                    param_dict = dict()\n",
        "                    param_dict['filters'] = filters\n",
        "                    param_dict['_padding'] = _padding\n",
        "                    param_dict['_kernel_initializer'] = _kernel_initializer\n",
        "                    param_dict['_activation'] = _activation\n",
        "                    param_dict['_optimizer'] = _optimizer\n",
        "                    param_dict['batch_normal'] = batch_normal\n",
        "                    param_dict['epochs'] = epochs\n",
        "                    param_dict['_kernel_size'] = _kernel_size\n",
        "                    param_dict['_strides'] = _strides\n",
        "                    param_dict['pool_type'] = pool_type\n",
        "                    # print(param_dict)\n",
        "                    params.append(param_dict)\n",
        "len(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv6FmRH5RJFK",
        "outputId": "c144c101-8605-465b-f6ff-d500df67d687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2304"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# datetime object containing current date and time\n",
        "now = datetime.now()\n",
        " \n",
        "print(\"now =\", now)\n",
        "\n",
        "# dd/mm/YY H:M:S\n",
        "dt_string = now.strftime(\"%Y%m%d%H%M\")\n",
        "print(\"date and time =\", dt_string)\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TTq4ChAI015",
        "outputId": "8d792397-e51a-4fbb-8407-2a10875bf0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now = 2022-05-28 11:59:27.224467\n",
            "date and time = 202228051159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# params = params[5:7]\n",
        "label_classes = [1, 2, 4]\n",
        "results_list = list()\n",
        "for label_class in label_classes:\n",
        "  train_feature_reshape, train_label = loadDatasetFromCSV('Traindata.csv', 'Trainlabel.csv', label_class)\n",
        "  class_cnt = len(np.unique(train_label))\n",
        "  # test\n",
        "  validation_feature_reshape, validation_label = loadDatasetFromCSV('Testdata.csv', 'Testlabel.csv', label_class)\n",
        "\n",
        "  train_dataset = preprocessDataset(train_feature_reshape, train_label)\n",
        "  validation_dataset = preprocessDataset(validation_feature_reshape, validation_label, shuffle_buffer_size=None)\n",
        "\n",
        "  results = list()          \n",
        "  for idx, param in enumerate(params):\n",
        "    param['_class_cnt'] = class_cnt\n",
        "    param['label_class'] = label_class\n",
        "    print('-'*5 +'[ '+ str(idx) + ' ]'+ '-'*5 + str(param))\n",
        "    results.append(model_fit(param, train_dataset, validation_dataset))\n",
        "  # save\n",
        "  with open(f'results_CNN_label_{label_class}_{dt_string}.pickle', 'wb') as f:\n",
        "    pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)\n",
        "  results_list = results_list + results\n"
      ],
      "metadata": {
        "id": "i1Tnuhc2x5eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save\n",
        "with open(f'results_CNN_{dt_string}.pickle', 'wb') as f:\n",
        "    pickle.dump(results_list, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAdPQV3EeZhO",
        "outputId": "3897b4fc-16da-4ab9-8b2c-da0e353d4aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://9747bf27-8aa7-4168-b9dc-7b00a406f415/assets\n",
            "INFO:tensorflow:Assets written to: ram://ab1328e4-c958-43b9-a909-85e328d58d25/assets\n",
            "INFO:tensorflow:Assets written to: ram://2024e3d0-87ad-469f-8ee8-ec26453132a7/assets\n",
            "INFO:tensorflow:Assets written to: ram://0e34876c-925a-47c2-8bc9-936d5fd52102/assets\n",
            "INFO:tensorflow:Assets written to: ram://4296993a-7b7f-400f-9ad7-7f2d856b5b81/assets\n",
            "INFO:tensorflow:Assets written to: ram://254d7a40-6e1e-4a21-868f-b00c6205a414/assets\n"
          ]
        }
      ]
    }
  ]
}